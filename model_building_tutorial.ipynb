{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ed55916",
   "metadata": {},
   "source": [
    "# üéì Complete Guide to Building Machine Learning Models\n",
    "\n",
    "Welcome! In this tutorial, you'll learn how to build, train, and evaluate machine learning models from scratch.\n",
    "\n",
    "## What You'll Learn:\n",
    "1. Data preparation and exploration\n",
    "2. Classification models (predicting categories)\n",
    "3. Regression models (predicting numbers)\n",
    "4. Clustering models (finding patterns)\n",
    "5. Model evaluation and selection\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ea64ed",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af96c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Classification Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Regression Models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Clustering Models\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.datasets import make_classification, make_regression, make_blobs\n",
    "\n",
    "# Settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0c2dc4",
   "metadata": {},
   "source": [
    "## üìä Part 1: Classification Models\n",
    "\n",
    "Classification is used when you want to predict **categories** (e.g., spam/not spam, disease/healthy).\n",
    "\n",
    "### Example: Customer Purchase Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7315e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample classification dataset\n",
    "# Let's predict whether a customer will make a purchase based on age and income\n",
    "X_class, y_class = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create a DataFrame for better understanding\n",
    "df_class = pd.DataFrame(X_class, columns=['Age (normalized)', 'Income (normalized)'])\n",
    "df_class['Purchase'] = y_class\n",
    "\n",
    "print(\"Dataset shape:\", df_class.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_class.head())\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df_class['Purchase'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b288ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(df_class['Age (normalized)'], \n",
    "                     df_class['Income (normalized)'], \n",
    "                     c=df_class['Purchase'], \n",
    "                     cmap='viridis', \n",
    "                     alpha=0.6,\n",
    "                     edgecolors='black')\n",
    "plt.xlabel('Age (normalized)', fontsize=12)\n",
    "plt.ylabel('Income (normalized)', fontsize=12)\n",
    "plt.title('Customer Data: Will They Purchase?', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, label='Purchase (0=No, 1=Yes)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92418034",
   "metadata": {},
   "source": [
    "### üîÑ Train-Test Split\n",
    "\n",
    "We split data into:\n",
    "- **Training set (80%)**: To teach the model\n",
    "- **Testing set (20%)**: To evaluate how well it learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e624e0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bb753d",
   "metadata": {},
   "source": [
    "### ü§ñ Model 1: Logistic Regression\n",
    "\n",
    "Simple but effective for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defff931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_lr:.2%}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cb3041",
   "metadata": {},
   "source": [
    "### üå≥ Model 2: Decision Tree\n",
    "\n",
    "Makes decisions like a flowchart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f701efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train\n",
    "dt_clf = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_dt = dt_clf.predict(X_test)\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(f\"Decision Tree Accuracy: {accuracy_dt:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7d7906",
   "metadata": {},
   "source": [
    "### üå≤ Model 3: Random Forest\n",
    "\n",
    "Combines multiple decision trees for better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1780095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02da3e8b",
   "metadata": {},
   "source": [
    "### üìä Compare Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fbd91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "models_comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest'],\n",
    "    'Accuracy': [accuracy_lr, accuracy_dt, accuracy_rf]\n",
    "})\n",
    "\n",
    "print(models_comparison)\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(models_comparison['Model'], models_comparison['Accuracy'], \n",
    "               color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Classification Models Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2%}',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bb23b0",
   "metadata": {},
   "source": [
    "### üéØ Confusion Matrix\n",
    "\n",
    "Shows where the model gets confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e8ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for best model (Random Forest)\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Purchase', 'Purchase'],\n",
    "            yticklabels=['No Purchase', 'Purchase'])\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.title('Confusion Matrix - Random Forest', fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTrue Negatives: {cm[0][0]}\")\n",
    "print(f\"False Positives: {cm[0][1]}\")\n",
    "print(f\"False Negatives: {cm[1][0]}\")\n",
    "print(f\"True Positives: {cm[1][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785a2d1a",
   "metadata": {},
   "source": [
    "## üìà Part 2: Regression Models\n",
    "\n",
    "Regression is used to predict **continuous values** (e.g., prices, temperatures, sales).\n",
    "\n",
    "### Example: House Price Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929815d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample regression dataset\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=500,\n",
    "    n_features=1,\n",
    "    noise=20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create DataFrame\n",
    "df_reg = pd.DataFrame(X_reg, columns=['House Size (sq ft)'])\n",
    "df_reg['Price ($1000s)'] = y_reg\n",
    "\n",
    "print(\"Dataset shape:\", df_reg.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_reg.head())\n",
    "print(\"\\nStatistics:\")\n",
    "print(df_reg.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061c548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_reg['House Size (sq ft)'], df_reg['Price ($1000s)'], \n",
    "           alpha=0.5, edgecolors='black')\n",
    "plt.xlabel('House Size (sq ft)', fontsize=12)\n",
    "plt.ylabel('Price ($1000s)', fontsize=12)\n",
    "plt.title('House Size vs Price', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d88d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train_reg)}\")\n",
    "print(f\"Testing samples: {len(X_test_reg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3286444",
   "metadata": {},
   "source": [
    "### üìè Linear Regression\n",
    "\n",
    "Fits a straight line through the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Predict\n",
    "y_pred_lin = lin_reg.predict(X_test_reg)\n",
    "\n",
    "# Evaluate\n",
    "mse_lin = mean_squared_error(y_test_reg, y_pred_lin)\n",
    "rmse_lin = np.sqrt(mse_lin)\n",
    "r2_lin = r2_score(y_test_reg, y_pred_lin)\n",
    "mae_lin = mean_absolute_error(y_test_reg, y_pred_lin)\n",
    "\n",
    "print(f\"Linear Regression Metrics:\")\n",
    "print(f\"  R¬≤ Score: {r2_lin:.4f} (closer to 1 is better)\")\n",
    "print(f\"  RMSE: {rmse_lin:.2f}\")\n",
    "print(f\"  MAE: {mae_lin:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf75f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test_reg, y_test_reg, alpha=0.5, label='Actual', edgecolors='black')\n",
    "plt.scatter(X_test_reg, y_pred_lin, alpha=0.5, label='Predicted', edgecolors='black')\n",
    "plt.plot(X_test_reg, y_pred_lin, color='red', linewidth=2, label='Regression Line')\n",
    "plt.xlabel('House Size (sq ft)', fontsize=12)\n",
    "plt.ylabel('Price ($1000s)', fontsize=12)\n",
    "plt.title('Linear Regression: Actual vs Predicted', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1603dc4d",
   "metadata": {},
   "source": [
    "### üå≥ Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353987d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train\n",
    "dt_reg = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "dt_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_dt_reg = dt_reg.predict(X_test_reg)\n",
    "r2_dt = r2_score(y_test_reg, y_pred_dt_reg)\n",
    "rmse_dt = np.sqrt(mean_squared_error(y_test_reg, y_pred_dt_reg))\n",
    "\n",
    "print(f\"Decision Tree Regressor Metrics:\")\n",
    "print(f\"  R¬≤ Score: {r2_dt:.4f}\")\n",
    "print(f\"  RMSE: {rmse_dt:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e69587c",
   "metadata": {},
   "source": [
    "### üìä Compare Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edbf03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "reg_comparison = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Decision Tree'],\n",
    "    'R¬≤ Score': [r2_lin, r2_dt],\n",
    "    'RMSE': [rmse_lin, rmse_dt]\n",
    "})\n",
    "\n",
    "print(reg_comparison)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# R¬≤ Score comparison\n",
    "axes[0].bar(reg_comparison['Model'], reg_comparison['R¬≤ Score'], \n",
    "            color=['#FF6B6B', '#4ECDC4'])\n",
    "axes[0].set_ylabel('R¬≤ Score', fontsize=11)\n",
    "axes[0].set_title('R¬≤ Score Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# RMSE comparison\n",
    "axes[1].bar(reg_comparison['Model'], reg_comparison['RMSE'], \n",
    "            color=['#FF6B6B', '#4ECDC4'])\n",
    "axes[1].set_ylabel('RMSE', fontsize=11)\n",
    "axes[1].set_title('RMSE Comparison (lower is better)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350d3f92",
   "metadata": {},
   "source": [
    "## üé® Part 3: Clustering Models\n",
    "\n",
    "Clustering finds **natural groups** in data without labels.\n",
    "\n",
    "### Example: Customer Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e637a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample clustering dataset\n",
    "X_cluster, y_cluster = make_blobs(\n",
    "    n_samples=300,\n",
    "    n_features=2,\n",
    "    centers=3,\n",
    "    cluster_std=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_cluster = pd.DataFrame(X_cluster, columns=['Annual Income', 'Spending Score'])\n",
    "\n",
    "print(\"Dataset shape:\", df_cluster.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_cluster.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e6f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize unlabeled data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_cluster['Annual Income'], df_cluster['Spending Score'], \n",
    "           alpha=0.6, edgecolors='black')\n",
    "plt.xlabel('Annual Income', fontsize=12)\n",
    "plt.ylabel('Spending Score', fontsize=12)\n",
    "plt.title('Customer Data (Unlabeled)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b84a5f5",
   "metadata": {},
   "source": [
    "### üéØ K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0516b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of clusters using Elbow Method\n",
    "inertias = []\n",
    "K_range = range(1, 10)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_cluster)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Plot Elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
    "plt.ylabel('Inertia', fontsize=12)\n",
    "plt.title('Elbow Method: Finding Optimal k', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Look for the 'elbow' point where the curve starts to flatten.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4175ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means with optimal k=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_cluster)\n",
    "\n",
    "# Add cluster labels to DataFrame\n",
    "df_cluster['Cluster'] = clusters\n",
    "\n",
    "print(\"Cluster distribution:\")\n",
    "print(df_cluster['Cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca675189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "scatter = plt.scatter(df_cluster['Annual Income'], \n",
    "                     df_cluster['Spending Score'],\n",
    "                     c=df_cluster['Cluster'],\n",
    "                     cmap='viridis',\n",
    "                     s=100,\n",
    "                     alpha=0.6,\n",
    "                     edgecolors='black')\n",
    "\n",
    "# Plot cluster centers\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], \n",
    "           c='red', s=300, alpha=0.8, \n",
    "           marker='*', edgecolors='black', linewidth=2,\n",
    "           label='Centroids')\n",
    "\n",
    "plt.xlabel('Annual Income', fontsize=12)\n",
    "plt.ylabel('Spending Score', fontsize=12)\n",
    "plt.title('Customer Segments (K-Means Clustering)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Successfully identified 3 customer segments!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c0b63",
   "metadata": {},
   "source": [
    "## üîÑ Part 4: Cross-Validation\n",
    "\n",
    "Cross-validation gives a more reliable estimate of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5708b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use our classification data\n",
    "models_cv = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'SVM': SVC(random_state=42)\n",
    "}\n",
    "\n",
    "cv_results = []\n",
    "\n",
    "for name, model in models_cv.items():\n",
    "    # 5-fold cross-validation\n",
    "    scores = cross_val_score(model, X_class, y_class, cv=5, scoring='accuracy')\n",
    "    cv_results.append({\n",
    "        'Model': name,\n",
    "        'Mean Accuracy': scores.mean(),\n",
    "        'Std Dev': scores.std()\n",
    "    })\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Accuracy: {scores.mean():.2%} (+/- {scores.std():.2%})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f61a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validation results\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "x_pos = np.arange(len(cv_df))\n",
    "plt.bar(x_pos, cv_df['Mean Accuracy'], \n",
    "        yerr=cv_df['Std Dev'], \n",
    "        capsize=5,\n",
    "        color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'],\n",
    "        edgecolor='black')\n",
    "plt.xticks(x_pos, cv_df['Model'], rotation=15, ha='right')\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('5-Fold Cross-Validation Results', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBest model based on cross-validation:\")\n",
    "best_idx = cv_df['Mean Accuracy'].idxmax()\n",
    "print(f\"{cv_df.loc[best_idx, 'Model']}: {cv_df.loc[best_idx, 'Mean Accuracy']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c7bd4",
   "metadata": {},
   "source": [
    "## üéØ Part 5: Feature Importance\n",
    "\n",
    "Understanding which features matter most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e078ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with named features\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Train a Random Forest\n",
    "rf_iris = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_iris.fit(X_iris, y_iris)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_iris.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(X_iris.shape[1]), importances[indices], \n",
    "        color='skyblue', edgecolor='black')\n",
    "plt.xticks(range(X_iris.shape[1]), \n",
    "          [feature_names[i] for i in indices], \n",
    "          rotation=45, ha='right')\n",
    "plt.ylabel('Importance', fontsize=12)\n",
    "plt.title('Feature Importance (Iris Dataset)', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature Ranking:\")\n",
    "for i, idx in enumerate(indices):\n",
    "    print(f\"{i+1}. {feature_names[idx]}: {importances[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da93b513",
   "metadata": {},
   "source": [
    "## üìù Summary & Key Takeaways\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **Classification Models**\n",
    "   - Logistic Regression: Simple, fast, interpretable\n",
    "   - Decision Trees: Easy to understand, can overfit\n",
    "   - Random Forest: More accurate, less interpretable\n",
    "   - SVM & KNN: Different approaches with their own strengths\n",
    "\n",
    "2. **Regression Models**\n",
    "   - Linear Regression: Best for linear relationships\n",
    "   - Decision Tree Regressor: Can capture non-linear patterns\n",
    "   \n",
    "3. **Clustering**\n",
    "   - K-Means: Find natural groups in data\n",
    "   - Elbow method: Determine optimal number of clusters\n",
    "\n",
    "4. **Model Evaluation**\n",
    "   - Train/Test Split: Basic validation\n",
    "   - Cross-Validation: More robust evaluation\n",
    "   - Multiple metrics: Accuracy, R¬≤, RMSE, MAE\n",
    "\n",
    "5. **Best Practices**\n",
    "   - Always split your data\n",
    "   - Use cross-validation for reliable results\n",
    "   - Compare multiple models\n",
    "   - Understand feature importance\n",
    "   - Visualize your results\n",
    "\n",
    "### Next Steps:\n",
    "- Try with your own datasets\n",
    "- Experiment with hyperparameter tuning\n",
    "- Learn about feature engineering\n",
    "- Explore deep learning for complex problems\n",
    "\n",
    "Happy Learning! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da63dd",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Practice Exercises\n",
    "\n",
    "Try these on your own:\n",
    "\n",
    "1. **Modify hyperparameters**: Change `max_depth` in Decision Trees and see how it affects accuracy\n",
    "2. **Try different splits**: Use 70-30 or 60-40 train-test splits\n",
    "3. **Add more features**: Generate datasets with more features and see how models perform\n",
    "4. **Real datasets**: Try with sklearn's built-in datasets (wine, breast_cancer, diabetes)\n",
    "5. **Ensemble methods**: Combine multiple models for better predictions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
